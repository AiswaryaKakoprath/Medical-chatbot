{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/amuchand47/Medical-Assistant-Chatbot/blob/master/ChatBot_using_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9wJhXUY6_J3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhwN0XQX4Icu"},"source":["# Chatbot using Seq2Seq LSTM models\n","In this notebook, we will assemble a seq2seq LSTM model using Keras Functional API to create a working Chatbot which would answer questions asked to it.\n","\n","Chatbots have become applications themselves. You can choose the field or stream and gather data regarding various questions. We can build a chatbot for an e-commerce webiste or a school website where parents could get information about the school.\n","\n","\n","So, let's start building our Chatbot.\n"]},{"cell_type":"markdown","metadata":{"id":"tm5g4WIG5ym2"},"source":["## 1) Importing the packages\n","\n","We will import [TensorFlow](https://www.tensorflow.org) and our beloved [Keras](https://www.tensorflow.org/guide/keras). Also, we import other modules which help in defining model layers.\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"UgZHR8TO0lFF"},"source":["import random\n","import json\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxiGOLldKOQD"},"source":["## 2) Preprocessing the data"]},{"cell_type":"markdown","metadata":{"id":"imkdw4os6FI4"},"source":["### A) Download the data\n","\n","The dataset hails from [chatterbot/english on Kaggle](https://www.kaggle.com/kausr25/chatterbotenglish).com by [kausr25](https://www.kaggle.com/kausr25). It contains pairs of questions and answers based on a number of subjects like food, history, AI etc.\n","\n","The raw data could be found from this repo -> https://github.com/shubham0204/Dataset_Archives\n"]},{"cell_type":"markdown","metadata":{"id":"nF1mDKD_R6Os"},"source":["### B) Reading the data from the files\n","\n","We parse each of the `.yaml` files.\n","\n","*   Concatenate two or more sentences if the answer has two or more of them.\n","*   Remove unwanted data types which are produced while parsing the data.\n","*   Append `<START>` and `<END>` to all the `answers`.\n","*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` ) into it.\n","\n","\n","\n"]},{"cell_type":"code","source":["from tensorflow.keras import preprocessing\n","import os\n","import json\n","from nltk.stem import WordNetLemmatizer\n","\n","# Load intents from JSON file\n","with open('/content/drive/MyDrive/Dataset/intents.json') as json_file:\n","    intents = json.load(json_file)\n","\n","# Initialize lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Preprocess data and extract words, classes, and documents\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = ['?', '!', '.', ',']\n","\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        pattern = pattern.lower()  # Convert pattern to lowercase\n","        word_list = nltk.word_tokenize(pattern)\n","        words.extend(word_list)\n","        documents.append((word_list, intent['tag']))\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n","\n","# Lemmatize words and remove ignored letters\n","words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n","\n","# Save words and classes\n","pickle.dump(words, open('/content/drive/MyDrive/Colab Notebooks/LSTM Medbot/words4.pkl', 'wb'))\n","pickle.dump(classes, open('/content/drive/MyDrive/Colab Notebooks/LSTM Medbot/classes4.pkl', 'wb'))\n","\n","directory_path = '/content/drive/MyDrive/Dataset/'\n","\n","# Check if the directory exists, if not, create it\n","if not os.path.exists(directory_path):\n","    os.makedirs(directory_path)\n","\n","# Filename to process\n","file_name = 'intents.json'\n","\n","# Construct the full file path\n","file_path = os.path.join(directory_path, file_name)\n","\n","questions = []\n","answers = []\n","\n","for filepath in files_list:\n","    stream = open(os.path.join(dir_path, filepath), 'rb')\n","    docs = yaml.safe_load(stream)\n","    conversations = docs['conversations']\n","    for con in conversations:\n","        if len(con) > 2:\n","            questions.append(con[0])\n","            replies = con[1:]\n","            ans = ' '.join(replies)\n","            answers.append(ans)\n","        elif len(con) > 1:\n","            questions.append(con[0])\n","            answers.append(con[1])\n","\n","answers_with_tags = [answer.strip() for answer in answers if isinstance(answer, str)]\n","questions = [question.strip() for question in questions]\n","\n","tokenizer = preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(questions + answers_with_tags)\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","print('VOCAB SIZE : {}'.format(VOCAB_SIZE))\n","\n"],"metadata":{"id":"Jn50eFS_GthO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VkbkuFk-Gtu1"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RzTBhga6MiV7"},"source":["from tensorflow.keras import preprocessing\n","import os\n","import yaml\n","\n","# Path to the directory containing files\n","directory_path = '/content/drive/MyDrive/Dataset/'\n","\n","# Check if the directory exists, if not, create it\n","if not os.path.exists(directory_path):\n","    os.makedirs(directory_path)\n","\n","# Filename to process\n","file_name = 'intents.json'\n","\n","# Construct the full file path\n","file_path = os.path.join(directory_path, file_name)\n","\n","# Check if the file exists\n","if os.path.isfile(file_path):\n","    # Read the file\n","    with open(file_path, 'rb') as file:\n","        docs = yaml.safe_load(file)\n","\n","        # Check for the correct key containing conversation data\n","        if 'conversations' in docs:\n","            conversations = docs['conversations']\n","            questions = []\n","            answers = []\n","\n","            # Process the conversations\n","            for con in conversations:\n","                if len(con) > 2:\n","                    questions.append(con[0])\n","                    answers.append(' '.join(con[1:]))\n","                elif len(con) > 1:\n","                    questions.append(con[0])\n","                    answers.append(con[1])\n","\n","            answers = ['<START> ' + ans + ' <END>' for ans in answers]\n","\n","            tokenizer = preprocessing.text.Tokenizer()\n","            tokenizer.fit_on_texts(questions + answers)\n","            VOCAB_SIZE = len(tokenizer.word_index) + 1\n","            print('VOCAB SIZE :', VOCAB_SIZE)\n","        else:\n","            print(\"Key 'conversations' not found in the YAML file.\")\n","\n","else:\n","    print(f\"File '{file_name}' does not exist in the directory '{directory_path}'.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mzsaO1YvS-M8"},"source":["\n","### C) Preparing data for Seq2Seq model\n","\n","Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n","\n","For `encoder_input_data` :\n","* Tokenize the `questions`. Pad them to their maximum length.\n","\n","For `decoder_input_data` :\n","* Tokenize the `answers`. Pad them to their maximum length.\n","\n","For `decoder_output_data` :\n","\n","* Tokenize the `answers`. Remove the first element from all the `tokenized_answers`. This is the `<START>` element which we added earlier.\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","tokenizer = Tokenizer()"],"metadata":{"id":"crVE9CgZDpA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["assert len(questions) > 0, \"The 'questions' variable is empty.\"\n","assert len(answers) >0, \"The 'answers' variable is empty.\""],"metadata":{"id":"bmj1F7oUENXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MY41TIlFENa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a5AD9ooQKc33"},"source":["\n","# encoder_input_data\n","tokenized_questions = tokenizer.texts_to_sequences( questions )\n","maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n","padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n","encoder_input_data = np.array( padded_questions )\n","print( encoder_input_data.shape , maxlen_questions )\n","\n","# decoder_input_data\n","tokenized_answers = tokenizer.texts_to_sequences( answers )\n","maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n","padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n","decoder_input_data = np.array( padded_answers )\n","print( decoder_input_data.shape , maxlen_answers )\n","\n","# decoder_output_data\n","tokenized_answers = tokenizer.texts_to_sequences( answers )\n","for i in range(len(tokenized_answers)) :\n","    tokenized_answers[i] = tokenized_answers[i][1:]\n","padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n","onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n","decoder_output_data = np.array( onehot_answers )\n","print( decoder_output_data.shape )\n","\n","# Saving all the arrays to storage\n","np.save( '/content/drive/MyDrive/Colab Notebooks/Chatbot/enc_in_data.npy' , encoder_input_data )\n","np.save( '/content/drive/MyDrive/Colab Notebooks/Chatbot/dec_in_data.npy' , decoder_input_data )\n","np.save( '/content/drive/MyDrive/Colab Notebooks/Chatbot/dec_tar_data.npy' , decoder_output_data )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SwY3T139l19"},"source":["## 3) Defining the Encoder-Decoder model\n","The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n","\n","\n","*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n","*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n","*   LSTM layer : Provide access to Long-Short Term cells.\n","\n","Working :\n","\n","1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ).\n","2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n","3.   These states are set in the LSTM cell of the decoder.\n","4.   The decoder_input_data comes in through the Embedding layer.\n","5.   The Embeddings goes in LSTM cell ( which had the states ) to produce seqeunces.\n","\n","**Important points :**\n","\n","\n","*   `200` is the output of the GloVe embeddings.\n","*   `embedding_matrix` is the GloVe embedding which we downloaded earlier.\n","\n","\n","<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n","\n","\n","Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-gUYtOwv21rt"},"source":["\n","encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n","encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n","encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n","encoder_states = [ state_h , state_c ]\n","\n","decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n","decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n","decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n","decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n","decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax )\n","output = decoder_dense ( decoder_outputs )\n","\n","model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n","model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n","\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n9g_8sR7WWf3"},"source":["## 4) Training the model\n","We train the model for a number of epochs with `RMSprop` optimizer and `categorical_crossentropy` loss function."]},{"cell_type":"code","metadata":{"id":"N74NZnfo3Id-"},"source":["\n","model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=75 )\n","model.save( 'model.h5' )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3sOLQr0M-lAe"},"source":["## 5) Defining inference models\n","We create inference models which help in predicting answers.\n","\n","**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n","\n","**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."]},{"cell_type":"code","metadata":{"id":"1u5DE4qo3Mf2"},"source":["\n","def make_inference_models():\n","\n","    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n","\n","    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n","    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n","\n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","    decoder_outputs, state_h, state_c = decoder_lstm(\n","        decoder_embedding , initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = tf.keras.models.Model(\n","        [decoder_inputs] + decoder_states_inputs,\n","        [decoder_outputs] + decoder_states)\n","\n","    return encoder_model , decoder_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rxZp0ZRy-6dA"},"source":["## 6) Talking with our Chatbot\n","\n","First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding.\n"]},{"cell_type":"code","metadata":{"id":"5P_wDD554q9O"},"source":["\n","def str_to_tokens( sentence : str ):\n","    words = sentence.lower().split()\n","    tokens_list = list()\n","    for word in words:\n","        tokens_list.append( tokenizer.word_index[ word ] )\n","    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djEPrfJBmZE-"},"source":["\n","\n","\n","1.   First, we take a question as input and predict the state values using `enc_model`.\n","2.   We set the state values in the decoder's LSTM.\n","3.   Then, we generate a sequence which contains the `<start>` element.\n","4.   We input this sequence in the `dec_model`.\n","5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n","6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer length.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"2zBmN8qB3O-e"},"source":["\n","enc_model , dec_model = make_inference_models()\n","\n","for _ in range(10):\n","    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n","    empty_target_seq = np.zeros( ( 1 , 1 ) )\n","    empty_target_seq[0, 0] = tokenizer.word_index['start']\n","    stop_condition = False\n","    decoded_translation = ''\n","    while not stop_condition :\n","        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n","        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n","        sampled_word = None\n","        for word , index in tokenizer.word_index.items() :\n","            if sampled_word_index == index :\n","                decoded_translation += ' {}'.format( word )\n","                sampled_word = word\n","\n","        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n","            stop_condition = True\n","\n","        empty_target_seq = np.zeros( ( 1 , 1 ) )\n","        empty_target_seq[ 0 , 0 ] = sampled_word_index\n","        states_values = [ h , c ]\n","\n","    print( decoded_translation )\n"],"execution_count":null,"outputs":[]}]}