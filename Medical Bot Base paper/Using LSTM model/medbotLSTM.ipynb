{"cells":[{"cell_type":"code","execution_count":61,"metadata":{"id":"QDAgS3iKaRIG"},"outputs":[],"source":["import random\n","import json\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from tensorflow.keras.models import Sequential\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1711445731113,"user":{"displayName":"Aiswarya Kakoprath","userId":"17760907333949482032"},"user_tz":-330},"id":"45gin-ywaRKy","outputId":"80a65abf-e813-4537-e660-62efdd30f104"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\aksha\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('wordnet')\n"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"Q2y0DpYLaRN4"},"outputs":[],"source":["# Load intents from JSON file\n","with open('intents.json') as json_file:\n","    intents = json.load(json_file)\n"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"c3n9vGuvaRRa"},"outputs":[],"source":["# Initialize lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","# Preprocess data and extract words, classes, and documents\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = ['?', '!', '.', ',']\n"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"b3S7xeYfaRWK"},"outputs":[],"source":["for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        pattern = pattern.lower()  # Convert pattern to lowercase\n","        word_list = nltk.word_tokenize(pattern)\n","        words.extend(word_list)\n","        documents.append((word_list, intent['tag']))\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"IaCwa8qKaRZt"},"outputs":[],"source":["# Lemmatize words and remove ignored letters\n","words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n","\n","# Save words and classes\n","pickle.dump(words, open('words4.pkl', 'wb'))\n","pickle.dump(classes, open('classes4.pkl', 'wb'))\n"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"HI1fLHNpb_fj"},"outputs":[],"source":["\n","# Create training data\n","training = []\n","output_empty = [0] * len(classes)\n","\n","for document in documents:\n","    bag = []\n","    word_patterns = document[0]\n","    for word in words:\n","        bag.append(1) if word in word_patterns else bag.append(0)\n","\n","    output_row = list(output_empty)\n","    output_row[classes.index(document[1])] = 1\n","    training.append([bag, output_row])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTpzl5crF8Xy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":68,"metadata":{"id":"isOGayoZb_iR"},"outputs":[],"source":["random.shuffle(training)\n","train_x = np.array([x[0] for x in training])\n","train_y = np.array([x[1] for x in training])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTRpjrsdtjlK"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":508,"status":"ok","timestamp":1711445672553,"user":{"displayName":"Aiswarya Kakoprath","userId":"17760907333949482032"},"user_tz":-330},"id":"4C49dFeI0uzU","outputId":"fbcfbbdd-d80c-4cc9-e1f6-d69bccd1ea95"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12397,"status":"ok","timestamp":1711447682159,"user":{"displayName":"Aiswarya Kakoprath","userId":"17760907333949482032"},"user_tz":-330},"id":"F5qGvtpkb_k_","outputId":"fd741fbd-c453-4f3c-d817-f834927c96c1"},"outputs":[{"name":"stderr","output_type":"stream","text":["<>:24: SyntaxWarning: invalid escape sequence '\\m'\n","<>:24: SyntaxWarning: invalid escape sequence '\\m'\n","C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_21520\\715718288.py:24: SyntaxWarning: invalid escape sequence '\\m'\n","  model.save('Using LSTM model\\medchatbot_lstm.h5')\n","C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_21520\\715718288.py:24: SyntaxWarning: invalid escape sequence '\\m'\n","  model.save('Using LSTM model\\medchatbot_lstm.h5')\n"]},{"ename":"ValueError","evalue":"Unrecognized keyword arguments passed to Embedding: {'input_length': 134}","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[70], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define LSTM model architecture\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m128\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39ml2(\u001b[38;5;241m0.01\u001b[39m), recurrent_regularizer\u001b[38;5;241m=\u001b[39ml2(\u001b[38;5;241m0.01\u001b[39m)))\n","File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     72\u001b[0m     input_dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n","File \u001b[1;32mc:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n","\u001b[1;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_length': 134}"]}],"source":["# Define LSTM model architecture\n","model = Sequential()\n","model.add(Embedding(len(words), 128, input_length=len(train_x[0])))\n","model.add(Dropout(0.5))\n","model.add(LSTM(128))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","\n","\n","# Optimize Training Parameters\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","# Compile model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Perform Error Analysis\n","history = model.fit(train_x, train_y, epochs=200, batch_size=32, verbose=1,\n","                    validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n","\n","\n","# Save model\n","model.save('Using LSTM model\\medchatbot_lstm.h5')\n","\n","print('Training Done')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvGbqQDc9W6-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiYN0Eo29W-k"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":1456,"status":"ok","timestamp":1711446845153,"user":{"displayName":"Aiswarya Kakoprath","userId":"17760907333949482032"},"user_tz":-330},"id":"dXJ-ErHab_oY","outputId":"6b6dc52d-2c86-4df1-d9f1-05f52af8862f"},"outputs":[],"source":["# Plot training history\n","import matplotlib.pyplot as plt\n","plt.plot(hist.history['accuracy'], label='accuracy')\n","plt.plot(hist.history['loss'], label='loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MTQF4iG9Vpe"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOnQbHuq3V5ClrzfDFKjM6B","mount_file_id":"1vZJOLdOoh4dyy6HjJ7egFMHq-h2DlvXP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
